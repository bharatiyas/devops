Some Hypervisor providers are: VMWare, Virtual Box. VM based virtualization do Hardware level virtualization/isolation because we install many guest OS 
on the same hardware.

Container based technologies provide Kernel/OS Level Isolation because many containers on the same host OS (having a container engine layer).
Using containers we can achieve runtime isolation.

Image: is a read-only template used to create DC. DI can have layers of other images. Images are stored in a D registry like Docker Hub.

Containers: C are lightweight and portable encapsulations of an environment in which to run an application. C are created from images.
C contain all the binaries and dependencies to run our application.

Registry: used to store DIs. You can have your own registry.

Repositries: Inside a registry, images are stored in repositories. Repositry is a collection of different DIs with the same name but different tags.
Each tag has a different version of the DI. 

Class:Object::Docker Image:DC

Registry = {}(Repositries)
Repositry = {})(DIs)

Docker Client  <---------->  Docker Daemon = Docket Engine/Server. It does all the heavy lifting like building, running and distributing your containers.
DC can connect to local/remote DD.

DC is the user interface to Docker. It accepts commands and sends them to DD. DC can be CLI or GUI (Ex Kitematic).

Docker Machine - is a lightweight Linux VM made especially to run DD on OS X or Windows.

Docker for Windows/Mac runs as natiive Windows/Mac application and manages a Linux VM internally. If you are using Linux you should install D natively.

Docker Machine allows you to install DD on Virtual hosts.

Docker Componse is used for defining and running multi-container docker applications. 

Docker Quickstart Terminal is a Docker client which takes input and sends it to the DD (running inside VM)

Install docker on Vagrant VM (ubuntu/xenial):
$ sudo apt-get install docker.io

Make sure your user has access to the Group named: docker-group:
$ sudo usermod -G docker-group <your_username>

if the above command does not work or even after executing this you get an error like 
"Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock"then your docker group is "docker". 
Then try:
$ sudo usermod -G docker <your_username>



This will install latest version with Ubuntu.


Official repositories are certified repositories by Docker. Users are encouraged to use official documentation because they are properly documented,
follow best practices, designed for common use cases and security updates are applied to them in a timely manner. For official repo we can get more
support from the community.

An image is identified by its repository name and tag (version). If tag is not specified then Docker will use the default tag i.e., latest.

When we want to create a C then D tries to find the DI in our local machine. If it finds one it uses that otherwise it downloads from remote registry.

A DI is made of list of read-only layers that represend file system differences. Image layers are stacked on top of each others to form a base for 
the Container's file system. Each layer references its parent layer. Image at the very bottom is called the base image. 
When we create a new C we create a thin read-write layer on top of the underlying stack. This layer is often called the writable container layer. All
changes to the container like creating a new file or modifying an existing file is done on this layer. So when we create a container out of an image,
this writable layer is added on top of the image. This is a major difference between a container and image. When a container is deleted, the writable
layer is also deleted and underlying image remains unchanged.
This means that multiple containers can access/share the same underlying image, and yet have their own data state (on their indpendent writable layer).

How to build a DI?
-----------------

Two ways:

1) Commit changes made in a DC.
2) Write a D file. The D file contains a set of instructions to assembel an DI. Instruction can be: what command to run after container starts, adding 
					some source code, etc. D will read the file to build an DI. Each instruction will add a new layer to the image. D file should be
					named as Dockerfile

					After executing an instruction in the D file, D spins a new temporary container from that image. It then executes the next 
					instruction on that temporary container. Executing the instruction creates a new layer. D commits this new layer to the image and
					removes the temporary container. 
					It then repeates the cycle of creating a new temporary container, executing next instruction, commiting the new layer to the image,
					and deleting previous temporary container.
					
					While creating the docker file we should try to reduce the number of layers to be created. You can do this by concatenating
					instructions by using '&&' and '\':
						RUN apt-update && apt-install -y \
						   git \
						   vim

					Another good practice is to mention sort arguments alphanumerically. Like mention the packages you want to install alphanumerically
						RUN apt-update && apt-install -y \
						   git \
						   python \
						   vim
						   
					CMD Instructions: Specifies what command you want to run when the container starts. If no CMD instructions are given, D will use
					the CMD I mentioned in the base image.
						CMD ["echo", "Hello World!!!!"]
						
					To override the CMD instruction we can mention arguments at docker run
						$ docker run sanjayfromgomia/debiangit echo "hello from dcker"

Docker Cache: Docker caches the layers it creates after exeution of each instruction. So next time if same sequence of instructions are executed then
D uses layer available in it cache to build the image, rather than building the new layer again. This reduces build time. You will see output:

	Step 1/4 : FROM debian:jessie
	---> 0a374bb127cf
	Step 2/4 : RUN apt-get update && apt-get install -y     git     python  vim
	---> Using cache
	---> aa4a28fe6ceb
	Step 3/4 : CMD echo Hello my dear friends....
	---> Using cache
	---> 4b27c0684489
					
COPY instruction: Copies new files and directories from build_context_path t file system f the container. 

ADD instruction: ADD is similar to copy. But it can also download a file froom internet and copy to the image. Can unpack compressed files.					
				 Prefer COPY over ADD unless sure.

'latest' tag: Images which are tagged latest will not be updated automatically when a newer version of the image is pushed to the repository. We should
			  avoid using latest tag. It’s easy to look at it and think that we are pulling the most update to date image. The safest way is just to 
			  version your tags. 

Following USER instruction ensures that we would be running the app server using admin user. You should always set the USER insruction in your 
Dockerfile to run your process as an unprivilleged user within the containers, otherwise your process would be running as root within the container. 
Because if a attacker breaks into your container then he will have root access to your VM because the uid would be same. Generally it is very hard to 
escalate to a VM root user because of the capacity restrictions enforced by docker.
But it is good practice to use a non-privileged user.

USER admin
			  
To set the working directory for any run CMD, ADD or COPY instructions that follow in the file. 
			  
DC Links: DCLs allows DCs can talk to each other without using ports. DCLs allows to discover other Cs and securely transfer data from one C to another.
		  So we have source container and recipient container. Links are estd. using container names. There is no need to expose any ports which is more
		  secure. So do not need to use any -p flag for the source container.
		  
		  When we use DLs, docker creates an entry in the /etc/hosts file of the recipient container for the source container. This can be verified by:
		  $ docker exec -it <recipient_container> bash
		  $ more /etc/hosts
		  ------
		  $exit
		  
		  $ docker inspect <source_container> | grep IP
		  
Docker Compose: Manual linking of containers and configuring services becomes impractical for large applications. Then we used Docker Compose.
				It is a tool for defining and running multi-container D apps. It is very handy to get docker environment up and running.
				It does DC orchestration like starting, linking and stopping the DCs.
				You use a compose (yaml) file to configure your application services.
				
				Version 1 of D Compose does not support D volumes and D networks features. Versoin 2,3 does. 
				D network feature allows DCs to be discovered by their names automatically. And any service can reach any other service by referring 
				its name. So linking is not required any more.
 				
				Volume is a designated directory in a container to persist data independent of C lifecycle. Used to share data between C and host VM.
				When we use Volumes, we do not need to give COPY instruction.
		  
Docker Networking: D uses the networking capabilities of the host machine to provide networking features to its containers. Once DD is installed on the
					host machine a "Bridge (docker 0)" is provisioned on the host, which is used to bridge the traffic between the DC and host machine.
					Each DC connects to the Bridge through its Container Network Interface. Containers connect to each other and outside world through
					this bridge network interface. Four types of D network:
					
					1) Closed Network / None Network - This n/w does not have any access to the outside world. None Network adds a container to a 
														container specific network stack. That C lack a C interface so its totally isolated. This kind
														of C is called Closed C.
														Provides highest level of n/w security because it cannot be reached from outside world.
														But it cannot be used if networking (like making http request to outside world) is required.
														
					2) Bridge Network - It is the most common network model in D. Default type of network in DC. All the Cs in the same bridge n/w are 
										connected with each other so, they can communicate with each other. 
										The Cs can connect to the outside world via the Bridge Network interface. it is created when DD is initialized.
										Since it is default we do not need to specify --net flag.
										We can ping 1 container from another (on the same bridge n/w)
										We can ping any outside n/w like www.google.com
										
										We can create a bridge n/w.
										
										By default different Bridge N/w are isolated from each other so C in bridge n/w cannot ping C in another bridge 
										n/w. But we can connect Bridge n/ws.
										
										It is most suitable when you want to setup a small network on a single host. 
										
					3) Host Network - It is the least protected model, adds C on top of host's n/w stack such that these Cs have full access to host
										interface. Hence these Cs are also called Open Cs. No isolation for the Open Cs hence they are unprotected.
										But this provides Open Cs with increased performance. Should be used only when justifiable.
										
					4) Overlay Network - When you want to create a multi-host n/w. D provides OOTB support for this through Overlay N/w driver. Needs
										D engine to be running in Swarm mode. It is widely used in Production. 

By default Docker Compose sets up a single Bridge network for your services. Each C will join the default n/w and is reachable by other containers on that 
n/w. Using docker-compose files we can define complex network, like:
	1) Seperate network for different kind of services. Example - db_network for DB Services
	2) Proxy network for frontend services
	3) And then application can talk to both the networks
	
		services:
		  proxy:
		    build: ./proxy
			networks:
			  - front_network
		  app
		    build: ./app
			networks:
			  - front_network
			  - back_network
         db
		    image: postgres
			networks:
			  - back_network
		  
We can create network isolation for n-tier apps.

Unit Testing: We can incorporate Unit Tests into DIs. A single image is used through dev, testing and prod, which greatly ensures reliability of our
				tests. 
				It increases the size of the image. If your test suite is not large then you can include them in the image.

Docker Machine: It a simple way to provision new VMs and run containers on top of them. It can:
				1) Provision new VMs
				2) Install Docker Engine/Server
				3) Configure Docker client 
				
				If provides different drivers for:
				1) Virtual Box
				2) AWS - Use DM to provision VMs in AWS cloud 
				3) Digital Ocean
				4) Google app engine